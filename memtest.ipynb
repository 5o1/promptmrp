{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000000, 100])\n",
      "Current Allocated Memory: 762.94 MB\n",
      "Current Reserved Memory: 3818.00 MB\n",
      "Peak Allocated Memory: 8011.40 MB\n",
      "Peak Reserved Memory: 8398.00 MB\n"
     ]
    }
   ],
   "source": [
    "from models.modules import rarrange\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "\n",
    "device = \"cuda:0\"\n",
    "\n",
    "class TinyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = torch.nn.Parameter(torch.tensor([0.]))\n",
    "    \n",
    "    def forward(self , x):\n",
    "        print(x.shape)\n",
    "        return x * self.net\n",
    "\n",
    "x = torch.randn(1000, 1000, 100).to(device)\n",
    "\n",
    "m = TinyModel().to(device).rearrange(\"h c q\", \"$ q\")\n",
    "\n",
    "x = m(x)\n",
    "\n",
    "\n",
    "def format_size(bytes, unit=\"MB\"):\n",
    "    \"\"\"\n",
    "    将字节数转换为可读单位。\n",
    "    Args:\n",
    "        bytes (int): 输入的字节数。\n",
    "        unit (str): 单位，可选值为 \"KB\", \"MB\", \"GB\"。默认是 \"MB\"。\n",
    "    Returns:\n",
    "        str: 格式化后的字符串。\n",
    "    \"\"\"\n",
    "    units = {\"KB\": 1 << 10, \"MB\": 1 << 20, \"GB\": 1 << 30}\n",
    "    if unit not in units:\n",
    "        raise ValueError(f\"Invalid unit '{unit}'. Valid units: {list(units.keys())}\")\n",
    "    return f\"{bytes / units[unit]:.2f} {unit}\"\n",
    "\n",
    "gc.collect()  # 强制执行垃圾回收\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 获取当前显存使用情况\n",
    "allocated = torch.cuda.memory_allocated()  # 当前分配的显存\n",
    "reserved = torch.cuda.memory_reserved()   # 当前预留的显存（缓存池大小）\n",
    "\n",
    "# 获取峰值显存使用量\n",
    "peak_allocated = torch.cuda.max_memory_allocated()  # 分配的显存峰值\n",
    "peak_reserved = torch.cuda.max_memory_reserved()   # 预留的显存峰值\n",
    "\n",
    "\n",
    "# 打印结果\n",
    "print(f\"Current Allocated Memory: {format_size(allocated, 'MB')}\")   # 当前已用显存\n",
    "print(f\"Current Reserved Memory: {format_size(reserved, 'MB')}\")    # 当前缓存池大小\n",
    "print(f\"Peak Allocated Memory: {format_size(peak_allocated, 'MB')}\")  # 峰值分配显存\n",
    "print(f\"Peak Reserved Memory: {format_size(peak_reserved, 'MB')}\")   # 峰值缓存池大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Allocated Memory: 2678.61 MB\n",
      "Current Reserved Memory: 2680.00 MB\n",
      "Peak Allocated Memory: 2678.61 MB\n",
      "Peak Reserved Memory: 2950.00 MB\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import torch\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import heapq\n",
    "\n",
    "\n",
    "path = \"/home/hulabdl/CMRxRecon2025/train/Cine@TrainingSet@FullSample@Center003@UIH_30T_umr880@P009@cine_sax.h5\"\n",
    "\n",
    "device = \"cuda:0\"\n",
    "\n",
    "class TinyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        self.net = torch.nn.Parameter(torch.tensor([0]))\n",
    "    \n",
    "    def forward(self , x):\n",
    "        return x * self.net\n",
    "\n",
    "\n",
    "with h5py.File(path, \"r\") as f:\n",
    "    ksp = f['kspace'][()][:,:,:,:,:]\n",
    "    ksp = torch.as_tensor(ksp).to(device)\n",
    "\n",
    "import gc\n",
    "\n",
    "gc.collect()  # 强制执行垃圾回收\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def format_size(bytes, unit=\"MB\"):\n",
    "    \"\"\"\n",
    "    将字节数转换为可读单位。\n",
    "    Args:\n",
    "        bytes (int): 输入的字节数。\n",
    "        unit (str): 单位，可选值为 \"KB\", \"MB\", \"GB\"。默认是 \"MB\"。\n",
    "    Returns:\n",
    "        str: 格式化后的字符串。\n",
    "    \"\"\"\n",
    "    units = {\"KB\": 1 << 10, \"MB\": 1 << 20, \"GB\": 1 << 30}\n",
    "    if unit not in units:\n",
    "        raise ValueError(f\"Invalid unit '{unit}'. Valid units: {list(units.keys())}\")\n",
    "    return f\"{bytes / units[unit]:.2f} {unit}\"\n",
    "\n",
    "# 获取当前显存使用情况\n",
    "allocated = torch.cuda.memory_allocated()  # 当前分配的显存\n",
    "reserved = torch.cuda.memory_reserved()   # 当前预留的显存（缓存池大小）\n",
    "\n",
    "# 获取峰值显存使用量\n",
    "peak_allocated = torch.cuda.max_memory_allocated()  # 分配的显存峰值\n",
    "peak_reserved = torch.cuda.max_memory_reserved()   # 预留的显存峰值\n",
    "\n",
    "# 打印结果\n",
    "print(f\"Current Allocated Memory: {format_size(allocated, 'MB')}\")   # 当前已用显存\n",
    "print(f\"Current Reserved Memory: {format_size(reserved, 'MB')}\")    # 当前缓存池大小\n",
    "print(f\"Peak Allocated Memory: {format_size(peak_allocated, 'MB')}\")  # 峰值分配显存\n",
    "print(f\"Peak Reserved Memory: {format_size(peak_reserved, 'MB')}\")   # 峰值缓存池大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(np.int64(175545600), '/home/hulabdl/CMRxRecon2025/train/Cine@TrainingSet@FullSample@Center003@UIH_30T_umr880@P009@cine_sax.h5', array([ 12,  10,  10, 446, 328])), (np.int64(166320000), '/home/hulabdl/CMRxRecon2025/train/Cine@TrainingSet@FullSample@Center003@UIH_30T_umr880@P025@cine_sax.h5', array([ 12,  10,  10, 450, 308])), (np.int64(151454400), '/home/hulabdl/CMRxRecon2025/train/Cine@TrainingSet@FullSample@Center003@UIH_30T_umr880@P008@cine_sax.h5', array([ 12,  10,  10, 454, 278])), (np.int64(149452800), '/home/hulabdl/CMRxRecon2025/train/Cine@TrainingSet@FullSample@Center003@UIH_30T_umr880@P024@cine_sax.h5', array([ 12,  10,  10, 448, 278])), (np.int64(149452800), '/home/hulabdl/CMRxRecon2025/train/Cine@TrainingSet@FullSample@Center003@UIH_30T_umr880@P015@cine_sax.h5', array([ 12,  10,  10, 448, 278])), (np.int64(149452800), '/home/hulabdl/CMRxRecon2025/train/Cine@TrainingSet@FullSample@Center003@UIH_30T_umr880@P011@cine_sax.h5', array([ 12,  10,  10, 448, 278])), (np.int64(148785600), '/home/hulabdl/CMRxRecon2025/train/Cine@TrainingSet@FullSample@Center003@UIH_30T_umr880@P017@cine_sax.h5', array([ 12,  10,  10, 446, 278])), (np.int64(146784000), '/home/hulabdl/CMRxRecon2025/train/Cine@TrainingSet@FullSample@Center003@UIH_30T_umr880@P006@cine_sax.h5', array([ 12,  10,  10, 440, 278])), (np.int64(145449600), '/home/hulabdl/CMRxRecon2025/train/Cine@TrainingSet@FullSample@Center003@UIH_30T_umr880@P020@cine_sax.h5', array([ 12,  10,  10, 436, 278])), (np.int64(142560000), '/home/hulabdl/CMRxRecon2025/train/Cine@TrainingSet@FullSample@Center001@UIH_30T_umr780@P029@cine_sax.h5', array([ 12,  11,  10, 450, 240]))]\n"
     ]
    }
   ],
   "source": [
    "path = \"/home/hulabdl/CMRxRecon2025/train/\"\n",
    "\n",
    "import h5py\n",
    "import torch\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import heapq\n",
    "\n",
    "device = \"cuda:0\"\n",
    "\n",
    "filelist = glob(path + \"*.h5\")\n",
    "\n",
    "lstnpts = []\n",
    "\n",
    "for file in filelist:\n",
    "    with h5py.File(file, \"r\") as f:\n",
    "        attrs = f.attrs\n",
    "        shape = attrs['shape']\n",
    "        npts = np.prod(shape)\n",
    "        lstnpts.append((npts, file, shape))\n",
    "\n",
    "print(heapq.nlargest(10, lstnpts, key=lambda x: x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(np.int64(633600), '/home/hulabdl/CMRxRecon2025/train/Mapping@TrainingSet@FullSample@Center006@Siemens_30T_Prisma@P012@T2map.h5', array([  3,   1,  10, 384,  55])), (np.int64(675120), '/home/hulabdl/CMRxRecon2025/train/Mapping@TrainingSet@FullSample@Center006@Siemens_30T_Prisma@P029@T2map.h5', array([  3,   1,  10, 388,  58])), (np.int64(741120), '/home/hulabdl/CMRxRecon2025/train/Mapping@TrainingSet@FullSample@Center006@Siemens_30T_Prisma@P032@T2map.h5', array([  3,   1,  10, 386,  64])), (np.int64(771840), '/home/hulabdl/CMRxRecon2025/train/Mapping@TrainingSet@FullSample@Center006@Siemens_30T_Prisma@P022@T2map.h5', array([  3,   1,  10, 384,  67])), (np.int64(802200), '/home/hulabdl/CMRxRecon2025/train/Mapping@TrainingSet@FullSample@Center006@Siemens_30T_Prisma@P031@T2map.h5', array([  3,   1,  10, 382,  70])), (np.int64(802200), '/home/hulabdl/CMRxRecon2025/train/Mapping@TrainingSet@FullSample@Center006@Siemens_30T_Prisma@P019@T2map.h5', array([  3,   1,  10, 382,  70])), (np.int64(826440), '/home/hulabdl/CMRxRecon2025/train/Mapping@TrainingSet@FullSample@Center006@Siemens_30T_Prisma@P015@T2map.h5', array([  3,   1,  10, 388,  71])), (np.int64(912000), '/home/hulabdl/CMRxRecon2025/train/Mapping@TrainingSet@FullSample@Center006@Siemens_30T_Prisma@P001@T2map.h5', array([  3,   1,  10, 380,  80])), (np.int64(921600), '/home/hulabdl/CMRxRecon2025/train/Mapping@TrainingSet@FullSample@Center006@Siemens_30T_Prisma@P025@T2map.h5', array([  3,   1,  10, 384,  80])), (np.int64(1008480), '/home/hulabdl/CMRxRecon2025/train/Mapping@TrainingSet@FullSample@Center006@Siemens_30T_Prisma@P011@T2map.h5', array([  3,   1,  10, 382,  88]))]\n"
     ]
    }
   ],
   "source": [
    "print(heapq.nsmallest(10, lstnpts, key=lambda x: x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 示例模型\n",
    "class ModelPart1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = torch.nn.Parameter(torch.tensor([0.]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.net\n",
    "\n",
    "class ModelPart2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = torch.nn.Parameter(torch.tensor([0.]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.net\n",
    "\n",
    "class ModelPart3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = torch.nn.Parameter(torch.tensor([0.]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x ** self.net\n",
    "\n",
    "# 初始化模型\n",
    "model_part1 = ModelPart1()\n",
    "model_part2 = ModelPart2()\n",
    "model_part3 = ModelPart3()\n",
    "\n",
    "# 优化器\n",
    "optimizer = optim.SGD(\n",
    "    list(model_part1.parameters()) +\n",
    "    list(model_part2.parameters()) +\n",
    "    list(model_part3.parameters()),\n",
    "    lr=0.01\n",
    ")\n",
    "\n",
    "# 输入数据\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "# 前向传播和后向传播\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# 第一部分\n",
    "out1 = model_part1(x)\n",
    "loss1 = (out1 - 2) ** 2\n",
    "loss1.backward()  # 第一次后向传播\n",
    "# 注意，这里不需要 retain_graph，因为第二部分的计算只依赖 out1\n",
    "\n",
    "# 第二部分\n",
    "out2 = model_part2(out1.detach())  # 显式切断梯度流\n",
    "loss2 = (out2 - 5) ** 2\n",
    "loss2.backward()  # 第二次后向传播\n",
    "\n",
    "# 第三部分\n",
    "out3 = model_part3(out2.detach())  # 再次切断梯度流\n",
    "loss3 = (out3 - 25) ** 2\n",
    "loss3.backward()  # 第三次后向传播\n",
    "\n",
    "# 更新参数\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gc\n",
    "\n",
    "device = \"cuda:0\"\n",
    "\n",
    "# 1. 定义模型 (简单的线性回归模型为例)\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.in_ = torch.nn.Conv2d(100, 50, 3, 1, 'same')\n",
    "\n",
    "        self.net = torch.nn.Conv2d(50, 50, 3, 1, 'same')\n",
    "\n",
    "        self.out = torch.nn.Conv2d(50, 100, 3, 1, 'same')\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.in_(x)\n",
    "\n",
    "        for _ in range(42):\n",
    "            x = self.net(x)\n",
    "\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class ModelContinue(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = torch.nn.Parameter(torch.tensor([0.]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.net\n",
    "\n",
    "m = SimpleModel().to(device)\n",
    "p = ModelContinue().to(device)\n",
    "\n",
    "x = torch.randn(50,100,100,100, device=\"cuda:0\", requires_grad=True)\n",
    "\n",
    "x = m(x)\n",
    "\n",
    "loss = x.mean()\n",
    "\n",
    "loss2 = p(loss.detach())\n",
    "\n",
    "loss2 = loss2.mean()\n",
    "\n",
    "print(loss, loss2)\n",
    "\n",
    "loss.backward()\n",
    "loss2.backward()\n",
    "\n",
    "# del x  # 删除张量\n",
    "gc.collect()  # 强制执行垃圾回收\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def format_size(bytes, unit=\"MB\"):\n",
    "    \"\"\"\n",
    "    将字节数转换为可读单位。\n",
    "    Args:\n",
    "        bytes (int): 输入的字节数。\n",
    "        unit (str): 单位，可选值为 \"KB\", \"MB\", \"GB\"。默认是 \"MB\"。\n",
    "    Returns:\n",
    "        str: 格式化后的字符串。\n",
    "    \"\"\"\n",
    "    units = {\"KB\": 1 << 10, \"MB\": 1 << 20, \"GB\": 1 << 30}\n",
    "    if unit not in units:\n",
    "        raise ValueError(f\"Invalid unit '{unit}'. Valid units: {list(units.keys())}\")\n",
    "    return f\"{bytes / units[unit]:.2f} {unit}\"\n",
    "\n",
    "# 获取当前显存使用情况\n",
    "allocated = torch.cuda.memory_allocated()  # 当前分配的显存\n",
    "reserved = torch.cuda.memory_reserved()   # 当前预留的显存（缓存池大小）\n",
    "\n",
    "# 获取峰值显存使用量\n",
    "peak_allocated = torch.cuda.max_memory_allocated()  # 分配的显存峰值\n",
    "peak_reserved = torch.cuda.max_memory_reserved()   # 预留的显存峰值\n",
    "\n",
    "# 打印结果\n",
    "print(f\"Current Allocated Memory: {format_size(allocated, 'MB')}\")   # 当前已用显存\n",
    "print(f\"Current Reserved Memory: {format_size(reserved, 'MB')}\")    # 当前缓存池大小\n",
    "print(f\"Peak Allocated Memory: {format_size(peak_allocated, 'MB')}\")  # 峰值分配显存\n",
    "print(f\"Peak Reserved Memory: {format_size(peak_reserved, 'MB')}\")   # 峰值缓存池大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "device = \"cuda:0\"\n",
    "\n",
    "# 1. 定义模型 (简单的线性回归模型为例)\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.linear = nn.Linear(1, 1)  # 输入1维，输出1维\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# 2. 准备数据\n",
    "# 创建一些简单的线性数据：y = 2x + 1\n",
    "x_train = torch.tensor([[1.0], [2.0], [3.0], [4.0]], requires_grad=False)\n",
    "y_train = torch.tensor([[3.0], [5.0], [7.0], [9.0]], requires_grad=False)\n",
    "\n",
    "# 3. 初始化模型、损失函数和优化器\n",
    "model = SimpleModel()\n",
    "criterion = nn.MSELoss()  # 均方误差损失\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)  # 随机梯度下降，学习率为0.01\n",
    "\n",
    "# 4. 训练循环\n",
    "num_epochs = 100  # 训练迭代次数\n",
    "for epoch in range(num_epochs):\n",
    "    # 前向传播: 计算预测值\n",
    "    y_pred = model(x_train)\n",
    "\n",
    "    # 计算损失\n",
    "    loss = criterion(y_pred, y_train)\n",
    "\n",
    "    # 反向传播: 计算梯度\n",
    "    optimizer.zero_grad()  # 清空之前的梯度\n",
    "    loss.backward()        # 反向传播计算梯度\n",
    "\n",
    "    # 更新参数\n",
    "    optimizer.step()       # 应用梯度更新参数\n",
    "\n",
    "    # 打印训练进度\n",
    "    if (epoch + 1) % 10 == 0:  # 每10次打印一次\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# 5. 测试模型 (验证训练效果)\n",
    "with torch.no_grad():  # 禁用梯度计算\n",
    "    x_test = torch.tensor([[5.0]])\n",
    "    y_test_pred = model(x_test)\n",
    "    print(f'Prediction for input {x_test.item()}: {y_test_pred.item()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "promptmr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
