{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step1: 将多级目录的mat数据集整理成h5\n",
    "    parser = argparse.ArgumentParser(description='Prepare H5 dataset for CMRxRecon series dataset')\n",
    "    parser.add_argument('--output_h5_folder', type=str,\n",
    "                        default='/common/users/bx64/dataset/CMRxRecon2024/h5_dataset',\n",
    "                        help='path to save H5 dataset')\n",
    "    parser.add_argument('--input_matlab_folder', type=str,\n",
    "                        default='/common/users/bx64/dataset/CMRxRecon2024/home2/Raw_data/MICCAIChallenge2024/ChallengeData/MultiCoil',\n",
    "                        help='path to the original matlab data')\n",
    "    parser.add_argument('--split_json', type=str, default='configs/data_split/cmr24-cardiac.json', help='path to the split json file')\n",
    "    parser.add_argument('--year', type=int, required=True, choices=[2024, 2023, 2025], help='year of the dataset')\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    save_folder = args.output_h5_folder\n",
    "    mat_folder = args.input_matlab_folder\n",
    "    year = args.year\n",
    "    split_json = args.split_json\n",
    "    \n",
    "    print('matlab data folder: ', mat_folder)\n",
    "    print('h5 save folder: ', save_folder)\n",
    "\n",
    "    if not os.path.exists(save_folder):\n",
    "        os.makedirs(save_folder)\n",
    "        \n",
    "    print('## step 1: convert matlab training dataset to h5 dataset')\n",
    "\n",
    "    file_list = sorted(glob.glob(join(mat_folder, '*/TrainingSet/FullSample/Center*/*/P*/*.mat')))\n",
    "    print('number of total matlab files: ', len(file_list))\n",
    "    \n",
    "    # check if cuda is available\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:1')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    for ff in tqdm(file_list):\n",
    "        ##* get info from path\n",
    "        match = re.search(r'MultiCoil/([^/]+)/([^/]+)/([^/]+)/([^/]+)/([^/]+)/([^/]+)', ff)\n",
    "        modal = match.group(1)\n",
    "        TrainingSet = match.group(2)\n",
    "        FullSample = match.group(3)\n",
    "        center = match.group(4)\n",
    "        mridevice = match.group(5)\n",
    "        paid = match.group(6)\n",
    "        directory, filename = os.path.split(ff)  # 分割为目录和文件名\n",
    "        fid = os.path.basename(directory)  # 获取上一级目录名\n",
    "        ftype = os.path.splitext(filename)[0]  # 获取文件名（不含扩展名）\n",
    "        save_name = f'{modal}@{TrainingSet}@{FullSample}@{center}@{mridevice}@{paid}@{ftype}'\n",
    "        \n",
    "        ##*remove bad files\n",
    "        if remove_bad_files(save_name) and year == 2024:\n",
    "            continue\n",
    "\n",
    "        ##* load kdata\n",
    "        kdata = load_kdata(ff)\n",
    "        \n",
    "        ## transpose if the format of shape is matlab style\n",
    "        if kdata.shape[0] > 100:\n",
    "            kdata = kdata.transpose(tuple(range(kdata.ndim)[::-1]))\n",
    "\n",
    "        ##* swap phase_encoding and readout\n",
    "        kdata = kdata.swapaxes(-1,-2)\n",
    "        \n",
    "        ##* remove bad slices\n",
    "        if year == 2024:\n",
    "            kdata = remove_bad_slices(kdata, save_name)\n",
    "        \n",
    "        ##* get rss from kdata\n",
    "        kdata_th = to_tensor(kdata)\n",
    "        img_coil = ifft2c(kdata_th).to(device)\n",
    "        img_rss = rss_complex(img_coil, dim=-3).cpu().numpy()\n",
    "\n",
    "        ##* save h5\n",
    "        file = h5py.File(join(save_folder, save_name + '.h5'), 'w')\n",
    "        file.create_dataset('kspace', data=kdata)\n",
    "        file.create_dataset('reconstruction_rss', data=img_rss)\n",
    "\n",
    "        file.attrs['max'] = img_rss.max()\n",
    "        file.attrs['norm'] = np.linalg.norm(img_rss)\n",
    "        file.attrs['acquisition'] = modal\n",
    "        file.attrs['shape'] = kdata.shape\n",
    "        file.attrs['padding_left'] = 0\n",
    "        file.attrs['padding_right'] = kdata.shape[-1]\n",
    "        file.attrs['encoding_size'] = (kdata.shape[-2],kdata.shape[-1],1)\n",
    "        file.attrs['recon_size'] = (kdata.shape[-2],kdata.shape[-1],1)\n",
    "        file.attrs['patient_id'] = paid\n",
    "        file.attrs['center'] = center\n",
    "        file.attrs['mridevice'] = mridevice\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集分割已完成，结果已保存到 configs/data_split/cmr25-cardiac.json\n"
     ]
    }
   ],
   "source": [
    "# step2: 生成数据集分割json\n",
    "\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def generate_dataset_split_json(input_h5file_path, output_json_path=\"dataset_split.json\", test_size=0.2, random_seed=42):\n",
    "    \"\"\"\n",
    "    根据当前路径下的 .h5 文件生成数据集分割的 JSON 文件。\n",
    "    \n",
    "    Args:\n",
    "        output_json_path (str): 输出 JSON 文件的路径。\n",
    "        test_size (float): 验证集占比（默认为 0.2，即 20%）。\n",
    "        random_seed (int): 随机种子（默认为 42，保证可复现性）。\n",
    "    \"\"\"\n",
    "    # 获取当前路径下所有 .h5 文件的路径\n",
    "    h5_files = glob.glob(os.path.join(input_h5file_path, \"*.h5\"))\n",
    "    \n",
    "    # 检查是否有 .h5 文件\n",
    "    if not h5_files:\n",
    "        print(\"当前路径下没有找到任何 .h5 文件！\")\n",
    "        return\n",
    "\n",
    "    # 使用 train_test_split 进行数据集划分\n",
    "    train_files, val_files = train_test_split(h5_files, test_size=test_size, random_state=random_seed)\n",
    "\n",
    "    # 构造 JSON 数据\n",
    "    dataset_split = {\n",
    "        \"train\": train_files,\n",
    "        \"val\": val_files\n",
    "    }\n",
    "\n",
    "    # 将分割结果写入 JSON 文件\n",
    "    with open(output_json_path, \"w\") as json_file:\n",
    "        json.dump(dataset_split, json_file, indent=4)\n",
    "\n",
    "    print(f\"数据集分割已完成，结果已保存到 {output_json_path}\")\n",
    "\n",
    "# 调用函数\n",
    "generate_dataset_split_json(input_h5file_path=\"/home/hulabdl/CMRxRecon2025/h5_dataset\",\n",
    "                            output_json_path=\"configs/data_split/cmr25-cardiac.json\", test_size=0.2, random_seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step3: 根据json生成软连接\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "from os.path import join, split, realpath\n",
    "\n",
    "def create_symbolic_links(split_json_path, save_folder):\n",
    "    \"\"\"\n",
    "    根据给定的 JSON 文件（包含训练集和验证集文件名），\n",
    "    在目标文件夹内创建训练集和验证集的符号链接。\n",
    "\n",
    "    参数：\n",
    "        split_json_path (str): 包含训练集和验证集文件名的 JSON 文件路径。\n",
    "        save_folder (str): 包含 .h5 文件的目录路径。\n",
    "\n",
    "    \"\"\"\n",
    "    # 读取 JSON 文件\n",
    "    with open(split_json_path, 'r', encoding=\"utf-8\") as f:\n",
    "        split_dict = json.load(f)\n",
    "\n",
    "    print('JSON 文件中训练文件数量: ', len(split_dict['train']))\n",
    "    print('JSON 文件中验证文件数量: ', len(split_dict['val']))\n",
    "\n",
    "    # 创建训练和验证文件夹\n",
    "    train_save_folder = join(save_folder, 'train')\n",
    "    val_save_folder = join(save_folder, 'val')\n",
    "    if not os.path.exists(train_save_folder):\n",
    "        os.makedirs(train_save_folder)\n",
    "    if not os.path.exists(val_save_folder):\n",
    "        os.makedirs(val_save_folder)\n",
    "\n",
    "    # 创建符号链接\n",
    "    for f in split_dict['train']:\n",
    "        # 根据f获取绝对路径\n",
    "        f = realpath(f)\n",
    "        save_name = split(f)[-1]\n",
    "        os.symlink(f, join(train_save_folder, save_name))\n",
    "\n",
    "    for f in split_dict['val']:\n",
    "        f = realpath(f)\n",
    "        save_name = split(f)[-1]\n",
    "        os.symlink(f, join(val_save_folder, save_name))\n",
    "\n",
    "    # 打印结果\n",
    "    print('完成！')\n",
    "    print('训练集文件夹中的符号链接文件数量: ', len(glob.glob(join(train_save_folder, '*.h5'))))\n",
    "    print('验证集文件夹中的符号链接文件数量: ', len(glob.glob(join(val_save_folder, '*.h5'))))\n",
    "\n",
    "create_symbolic_links(\"configs/data_split/cmr25-cardiac.json\", \"/home/hulabdl/CMRxRecon2025/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "promptmr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
