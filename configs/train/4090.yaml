seed_everything: 42
trainer:
  accelerator: gpu
  strategy: ddp
  devices: [1,2,3,4,5,6,7]
  num_nodes: 1
  max_epochs: 1
  gradient_clip_val: 0.01
  log_every_n_steps: 50 # log every n steps during training
  deterministic: false
  use_distributed_sampler: false # use customized distributed sampler defined in data module while validation
  logger: 
    class_path: lightning.pytorch.loggers.WandbLogger
    init_args:
      project: cmr25
      mode: online
      save_dir: /home/hulabdl/PromptMR-plus/exp # path to save experiment and checkpoint
      tags: [cmr25-cardiac]
      name: cmr25-cardiac-pmr-plus
  callbacks:
    -
      class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        monitor: validation_loss
        mode: min 
        save_top_k: 0 # -1 to save all checkpoints, 5 to save the top 5 checkpoints
        save_last: True # always save the last checkpoint
        verbose: True # print checkpoint information
    # - 
    #   class_path: lightning.pytorch.callbacks.LearningRateMonitor
    #   init_args:
    #     logging_interval: 'epoch'
    #     log_momentum: true
    #     log_weight_decay: true